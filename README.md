# T5-Refiner-DomainFocus
# T5-é¢„å¤„ç†ç‰¹å®šè¯è¯­é®ç…§å¢å¼º

ï¼ˆCode will be uploaded later.ï¼‰

![Views](https://komarev.com/ghpvc/?username=llap4585&repo=T5-Refiner-DomainFocus&label=Project%20Views&color=blue&style=flat-square)

---
[â­ï¸English](#english) | [â­ï¸ä¸­æ–‡](#chinese)


[æ—¥æœ¬èª](#japanese) | [Deutsch](#deutsch) | [FranÃ§ais](#francais) | [EspaÃ±ol](#espanol) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](#hindi) | [í•œêµ­ì–´](#korean) | [PortuguÃªs](#portuguese)

---

[Demo](#Demo) 

[Requirements](#Requirements)

[References](#References)

[Privacy](#Privacy)

---
<a name="english"></a>
## â­ï¸ English

**T5-Refiner-DomainFocus** aims to empower models with intrinsic "semantic resilience" through **pre-training stage** strategy optimization, **enabling more robust handling of text corruption and the injection of domain-specific expertise.**

### ğŸ“– Project Background
During **the digitization of medical records**, **OCR (Optical Character Recognition)** often suffers from "character defects" in core terminology due to damaged paper, stamp occlusion, or other physical factors.
Traditional **T5** or **mT5** models (collectively referred to as T5) face two major challenges when processing such corrupted text:
* Limitations of Random Masking: The model learns to "guess" words based on sub-word roots rather than truly understanding complete medical concepts.
* Tokenization Misalignment: When letters are missing from a term, the tokenizer breaks it into meaningless fragments, causing the model to lose its semantic focus.

### âœ… Core Features
This project enhances model capability by optimizing the data preprocessing pipeline rather than relying on complex hard-coded rules:

* Expert Lexicon-Guided Atomic Masking:
By leveraging custom lexicons, the model is forced to treat professional terms (e.g., Acute Anterior Myocardial Infarction) as indivisible units during masking. This forces the model to derive answers from contextual logic rather than taking shortcuts via residual characters.

* Manual Enhanced Training:
Supports manual adjustment of masking probabilities for specific high-difficulty terms (ğŸ’¡Recommended: 50%-70%, not to exceed 80%), while simultaneously increasing the global masking rate (20%-25%).

* Automatic Punctuation Avoidance:
Prevents the introduction of noise interference during the masking process.

By creating scenarios of "extreme information loss," the model is compelled to maintain accurate reconstruction of professional semantics even under the worst input conditions.

### â—ï¸ Training Notes
* Preventing Early Stopping: After preprocessing, T5 models may exhibit slow loss reduction or local fluctuations, which can trick systems into stopping training prematurely.
* Convergence Judgment: It is recommended to extend training duration and evaluate convergence based on whether the loss decreases steadily across multiple stages. Insufficient training will significantly degrade restoration performance.

### ğŸ“Š Evaluation
Based on preliminary testing with the mT5-base standard model:
* Standard Model Performance: The restoration rate for specialized terminology is estimated to be below 60%. The remaining 40% of results are often logically incoherent and unacceptable for professional use.
* With DomainFocus Improvement: The estimated restoration rate reaches 85%. Of the remaining 15% error margin, most are semantic synonyms, which greatly improves the overall readability and logical consistency of the text.

### âš ï¸ Limitations
* Context Fragmentation: Due to the limited sequence length and the restricted number of masks per segment, long documents may suffer from semantic disconnection during chunking. It is recommended to feed partial overlapping context back during re-training.
* Algorithmic Limits: Since T5 restoration is based on statistical probability, it is impossible to guarantee 100% accuracy when dealing with highly complex text.
* Domain Dependency: The restoration effectiveness is highly dependent on the coverage and depth of the predefined expert lexicon.

### ğŸŒŒ Future Roadmap
* Automatic Defect Sensing:
Utilizing "tokenization fragments" as implicit signals. When OCR recognition is severely misaligned, the model will automatically locate semantic ruptures via anomalies in the token sequence.
* Semantic Auto-Alignment:
Eliminating the need for manual anchor points to achieve end-to-end restoration of OCR-damaged text.

[Demo](#Demo)

---
<a name="chinese"></a>
## â­ï¸ä¸­æ–‡
**T5-Refiner-DomainFocus** æ—¨åœ¨é€šè¿‡**é¢„è®­ç»ƒé˜¶æ®µ**çš„ç­–ç•¥ä¼˜åŒ–ï¼Œèµ‹äºˆæ¨¡å‹ä¸€ç§å†…åœ¨çš„â€œè¯­ä¹‰éŸ§æ€§â€ï¼Œ**ä½¿å…¶èƒ½æ›´ç¨³å¥åœ°å¤„ç†æ–‡æœ¬ç¼ºæŸå’Œæ³¨å…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚**

### ğŸ“–é¡¹ç›®èƒŒæ™¯
åœ¨å¤„ç†**åŒ»å­¦æ¡£æ¡ˆæ•°å­—åŒ–æ—¶**ï¼Œ**OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰** å¸¸å› çº¸è´¨å—æŸã€å°ç« é®æŒ¡ç­‰åŸå› ï¼Œå¯¼è‡´æ ¸å¿ƒæœ¯è¯­å‡ºç°â€œå­—ç¬¦ç¼ºæŸâ€ã€‚
ä¼ ç»Ÿçš„ **T5** æˆ– **mT5** æ¨¡å‹(ç»Ÿç§°T5ï¼‰åœ¨å¤„ç†è¿™äº›å—æŸæ–‡æœ¬æ—¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š
* éšæœºé®è”½çš„å±€é™æ€§ï¼šå¯¼è‡´æ¨¡å‹åªå­¦ä¼šäº†æ ¹æ®è¯æ ¹â€œçŒœè¯â€ï¼Œè€Œæ²¡æœ‰çœŸæ­£ç†è§£å®Œæ•´çš„åŒ»å­¦æ¦‚å¿µã€‚
* åˆ†è¯é”™ä½é—®é¢˜ï¼šå½“æœ¯è¯­ä¸¢å¤±å­—æ¯æ—¶ï¼Œåˆ†è¯å™¨ä¼šå°†å…¶åˆ‡ç¢ä¸ºæ— æ„ä¹‰çš„ç¢ç‰‡ï¼Œå¯¼è‡´æ¨¡å‹å¤±å»è¯­ä¹‰é‡å¿ƒã€‚

### âœ…å½“å‰æ ¸å¿ƒåŠŸèƒ½
æœ¬é¡¹ç›®ç›®å‰ä¸ä¾èµ–å¤æ‚çš„ç¡¬ç¼–ç è§„åˆ™ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†æµç¨‹æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼š

* ä¸“å®¶è¯åº“å¼•å¯¼çš„åŸå­åŒ–é®è”½ï¼š
ä¾æ‰˜è‡ªå®šä¹‰è¯åº“ï¼Œå¼ºåˆ¶æ¨¡å‹å°†ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ï¼šæ€¥æ€§å‰å£å¿ƒè‚Œæ¢—æ­»ï¼‰è§†ä¸ºä¸å¯åˆ†å‰²çš„æ•´ä½“è¿›è¡Œé®è”½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè¿«ä½¿æ¨¡å‹ä»ä¸Šä¸‹æ–‡çš„é€»è¾‘ä¸­å¯»æ‰¾ç­”æ¡ˆï¼Œè€Œéé€šè¿‡æ®‹ä½™å­—ç¬¦æŠ•æœºå–å·§ã€‚

* äººå·¥è®¾å®šå¼ºåŒ–è®­ç»ƒï¼š
æ”¯æŒæ‰‹åŠ¨æé«˜ç‰¹å®šé«˜éš¾åº¦æœ¯è¯­çš„é®è”½æ¦‚ç‡ï¼ˆğŸ’¡æ¨èåœ¨50%-70%ï¼Œä¸å®œè¶…è¿‡80%ï¼‰ï¼ŒåŒæ—¶å¯åŒæ­¥æé«˜æ•´ä½“çš„é®è”½ç‡ï¼ˆ20%-25%ï¼‰ã€‚

* è‡ªåŠ¨è§„é¿æ ‡ç‚¹ç¬¦å·ï¼š
é˜²æ­¢å¼•å…¥å¹²æ‰°ã€‚

é€šè¿‡äººä¸ºåˆ¶é€ â€œæç«¯ä¿¡æ¯ç¼ºå¤±â€çš„åœºæ™¯ï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨æœ€å·®çš„è¾“å…¥æƒ…å†µä¸‹ä¾ç„¶èƒ½ä¿æŒå¯¹ä¸“ä¸šè¯­ä¹‰çš„å‡†ç¡®è¿˜åŸã€‚

### â—ï¸è®­ç»ƒæ³¨æ„äº‹é¡¹
* é˜²æ­¢æ¨¡å‹æå‰åœæ­¢ï¼šåœ¨é¢„å¤„ç†ä¹‹åï¼ŒT5 æ¨¡å‹å¯èƒ½ä¼šå‡ºç° Loss ä¸‹é™ç¼“æ…¢æˆ–äº§ç”Ÿå±€éƒ¨æ³¢åŠ¨çš„å‡è±¡ï¼Œå¯¼è‡´ç³»ç»Ÿé”™è¯¯åœ°æå‰åœæ­¢è®­ç»ƒã€‚
* æ”¶æ•›åˆ¤æ–­å»ºè®®ï¼šæ¨èå¢åŠ è®­ç»ƒæ—¶é•¿ï¼Œå¹¶æ ¹æ®å¤šä¸ªé˜¶æ®µçš„ Loss æ˜¯å¦æŒç»­ç¨³å®šä¸‹é™æ¥ç»¼åˆåˆ¤æ–­æ¨¡å‹æ”¶æ•›æƒ…å†µã€‚è‹¥è®­ç»ƒæ—¶é—´ä¸è¶³ï¼Œè¿˜åŸæ•ˆæœå¯èƒ½ä¼šå¤§æ‰“æŠ˜æ‰£ã€‚

### ğŸ“Šæ•ˆæœè¯„ä¼°
æ ¹æ®åˆæ­¥æµ‹è¯•å¯¹æ¯”ï¼Œåœ¨ mT5-base æ ‡å‡†æ¨¡å‹ä¸­ï¼š
* æ ‡å‡†æ¨¡å‹è¡¨ç°ï¼šåœ¨ä¸“ä¸šé¢†åŸŸçš„è¯æ±‡è¿˜åŸç‡ä¼°ç®—åœ¨ 60% ä»¥ä¸‹ï¼Œå‰©ä½™ 40% çš„è¿˜åŸç»“æœé€»è¾‘æ··ä¹±ï¼Œå‡ ä¹æ— æ³•è¢«ä¸šåŠ¡æ¥å—ã€‚
* æœ¬é¡¹ç›®æ”¹è¿›åï¼šä¸“ä¸šè¯æ±‡è¿˜åŸç‡ä¼°ç®—è¾¾åˆ°äº† 85%ã€‚å‰©ä¸‹çš„ 15% è¯¯å·®ä¸­ï¼Œå¤§éƒ¨åˆ†æ˜¯è¯­ä¹‰ç›¸è¿‘çš„è¯æ±‡æ›¿ä»£ï¼Œæå¤§åœ°æé«˜äº†æ–‡æœ¬çš„æ•´ä½“å¯è¯»æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚

### âš ï¸ä½¿ç”¨é™åˆ¶
* ä¸Šä¸‹æ–‡ç‰‡æ®µåŒ–é™åˆ¶ï¼šç”±äºæ¨¡å‹å•æ¬¡å¤„ç†çš„æ–‡æœ¬é•¿åº¦æœ‰é™ï¼Œä¸”æ¯æ®µæ–‡æœ¬å†…æ ‡è®°ï¼ˆMaskï¼‰çš„è¯æ±‡æ•°é‡å—é™ï¼Œé•¿æ–‡æ¡£åœ¨åˆ‡åˆ†å¤„ç†æ—¶å¯èƒ½å­˜åœ¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–­è£‚çš„æƒ…å†µï¼Œå¯¼è‡´éƒ¨åˆ†è·¨æ®µè½çš„è¯­ä¹‰æ— æ³•è¢«å®Œç¾æ•æ‰ã€‚æ¨èå›ä¼ éƒ¨åˆ†ä¸Šä¸‹æ–‡å†è®­ç»ƒã€‚
* ç®—æ³•å±€é™æ€§ï¼šç”±äº T5 æ¨¡å‹æœ¬èº«çš„è¿˜åŸæ˜¯åŸºäºç»Ÿè®¡æ¦‚ç‡ç®—æ³•çš„ï¼Œå› æ­¤åœ¨å¤„ç†å¤æ‚çš„æ–‡æœ¬æ—¶ï¼Œä¸å¯èƒ½ä¿è¯ 100% çš„è¿˜åŸå‡†ç¡®ç‡ã€‚
* é¢†åŸŸä¾èµ–ï¼šè¿˜åŸæ•ˆæœé«˜åº¦ä¾èµ–äºé¢„è®¾ä¸“å®¶è¯åº“çš„è¦†ç›–é¢ä¸æ·±åº¦ã€‚

### ğŸŒŒæœªæ¥å¼€å‘è®¡åˆ’
* è‡ªåŠ¨ç¼ºæŸæ„ŸçŸ¥:
åˆ©ç”¨åˆ†è¯å™¨çš„â€œå¼‚å¸¸ç¢ç‰‡â€ä½œä¸ºéšæ€§ä¿¡å·ã€‚å½“ OCR è¯†åˆ«å‡ºç°ä¸¥é‡é”™ä½æ—¶ï¼Œæ¨¡å‹èƒ½é€šè¿‡åˆ†è¯åºåˆ—çš„å¼‚å¸¸æ³¢åŠ¨ï¼Œè‡ªåŠ¨å®šä½åˆ°è¯­ä¹‰æ–­è£‚å¤„ã€‚
* è¯­ä¹‰è‡ªåŠ¨å¯¹é½:
æ— éœ€äººå·¥æŒ‡å®šè¡”æ¥ç‚¹ï¼Œå®ç°æ¨¡å‹å¯¹ OCR æŸåæ–‡æœ¬çš„ç«¯åˆ°ç«¯ä¿®å¤ã€‚

[Demo](#Demo) 

---
<a name="japanese"></a>
## æ—¥æœ¬èªï¼ˆæ©Ÿæ¢°ç¿»è¨³ï¼‰
**T5-Refiner-DomainFocus** ã¯ã€**äº‹å‰å­¦ç¿’æ®µéš**ã®æˆ¦ç•¥çš„æœ€é©åŒ–ã‚’é€šã˜ã¦ã€ãƒ¢ãƒ‡ãƒ«ã«å›ºæœ‰ã®ã€Œæ„å‘³çš„å¼¾åŠ›æ€§ï¼ˆSemantic Resilienceï¼‰ã€ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€**ãƒ†ã‚­ã‚¹ãƒˆã®æ¬ æã‚’ã‚ˆã‚Šå …ç‰¢ã«å‡¦ç†ã—ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å°‚é–€çŸ¥è­˜ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚**

### ğŸ“–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆèƒŒæ™¯
**åŒ»ç™‚ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–**ã«ãŠã„ã¦ã€**OCRï¼ˆå…‰å­¦æ–‡å­—èªè­˜ï¼‰** ã¯ã€ç´™ã®æå‚·ã‚„å°å½±ã®é‡ãªã‚Šãªã©ã®åŸå› ã«ã‚ˆã‚Šã€æ ¸å¿ƒçš„ãªç”¨èªã«ã€Œæ–‡å­—æ¬ æã€ãŒç”Ÿã˜ã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚
å¾“æ¥ã® **T5** ã¾ãŸã¯ **mT5** ãƒ¢ãƒ‡ãƒ«ï¼ˆç·ç§°ã—ã¦T5ï¼‰ã¯ã€ã“ã‚Œã‚‰ã®æå‚·ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹éš›ã«2ã¤ã®ä¸»è¦ãªå•é¡Œã‚’æŠ±ãˆã¦ã„ã¾ã™ï¼š
* ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚­ãƒ³ã‚°ã®é™ç•Œï¼šãƒ¢ãƒ‡ãƒ«ãŒèªæ ¹ã«åŸºã¥ã„ãŸã€Œå˜èªã®æ¨æ¸¬ã€ã‚’å­¦ç¿’ã™ã‚‹ã«ã¨ã©ã¾ã‚Šã€å®Œå…¨ãªåŒ»ç™‚æ¦‚å¿µã‚’çœŸã«ç†è§£ã§ããªã„ã€‚
* ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®ä¸ä¸€è‡´å•é¡Œï¼šç”¨èªã®æ–‡å­—ãŒæ¬ è½ã™ã‚‹ã¨ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒãã‚Œã‚’ç„¡æ„å‘³ãªæ–­ç‰‡ã«ç´°åˆ†åŒ–ã—ã¦ã—ã¾ã„ã€ãƒ¢ãƒ‡ãƒ«ãŒæ„å‘³ã®é‡å¿ƒã‚’å¤±ã†ã€‚

### âœ…ç¾åœ¨ã®ã‚³ã‚¢æ©Ÿèƒ½
æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ç¾åœ¨ã€è¤‡é›‘ãªãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ«ãƒ¼ãƒ«ã«ä¾å­˜ã›ãšã€ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’å¼·åŒ–ã—ã¦ã„ã¾ã™ï¼š

* å°‚é–€ç”¨èªé›†ã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚‹ã‚¢ãƒˆãƒŸãƒƒã‚¯ãƒ»ãƒã‚¹ã‚­ãƒ³ã‚°ï¼š
ã‚«ã‚¹ã‚¿ãƒ ç”¨èªé›†ã«åŸºã¥ãã€å°‚é–€ç”¨èªï¼ˆä¾‹ï¼šæ€¥æ€§å‰å£å¿ƒç­‹æ¢—å¡ï¼‰ã‚’åˆ†å‰²ä¸å¯èƒ½ãªä¸€ä½“ã¨ã—ã¦å¼·åˆ¶çš„ã«ãƒã‚¹ã‚­ãƒ³ã‚°ã—ã¾ã™ã€‚ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€æ®‹å­˜æ–‡å­—ã‹ã‚‰ã®æ†¶æ¸¬ã§ã¯ãªãã€æ–‡è„ˆã®è«–ç†ã‹ã‚‰ç­”ãˆã‚’è¦‹ã¤ã‘å‡ºã™ã‚ˆã†ãƒ¢ãƒ‡ãƒ«ã«å¼·åˆ¶ã—ã¾ã™ã€‚

* ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚£ã‚·ãƒ£ãƒ«ãƒ»ã‚»ãƒƒãƒ†ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹å¼·åŒ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š
ç‰¹å®šã®é«˜é›£åº¦ç”¨èªã®ãƒã‚¹ã‚­ãƒ³ã‚°ç¢ºç‡ã‚’æ‰‹å‹•ã§é«˜ã‚ã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆğŸ’¡æ¨å¥¨50%-70%ã€80%ã‚’è¶…ãˆãªã„ã“ã¨ï¼‰ã€‚åŒæ™‚ã«ã€å…¨ä½“ã®ãƒã‚¹ã‚­ãƒ³ã‚°ç‡ï¼ˆ20%-25%ï¼‰ã‚’åŒæœŸã—ã¦é«˜ã‚ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

* å¥èª­ç‚¹ã®è‡ªå‹•å›é¿ï¼š
ãƒã‚¤ã‚ºã®æ··å…¥ã‚’é˜²æ­¢ã—ã¾ã™ã€‚

æ„å›³çš„ã«ã€Œæ¥µç«¯ãªæƒ…å ±ã®æ¬ è½ã€ã‚·ãƒ¼ãƒ³ã‚’ä½œã‚Šå‡ºã™ã“ã¨ã§ã€æœ€æ‚ªã®å…¥åŠ›çŠ¶æ³ä¸‹ã§ã‚‚å°‚é–€çš„ãªæ„å‘³ã‚’æ­£ç¢ºã«å¾©å…ƒã§ãã‚‹èƒ½åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«å¼·åˆ¶ã—ã¾ã™ã€‚

### â—ï¸è¨“ç·´ä¸Šã®æ³¨æ„ç‚¹
* ãƒ¢ãƒ‡ãƒ«ã®æ—©æœŸåœæ­¢ã®é˜²æ­¢ï¼šå‰å‡¦ç†å¾Œã€T5ãƒ¢ãƒ‡ãƒ«ã¯æå¤±ï¼ˆLossï¼‰ã®ä¸‹è½ãŒç·©ã‚„ã‹ã«ãªã£ãŸã‚Šã€å±€æ‰€çš„ãªå¤‰å‹•ãŒç”Ÿã˜ãŸã‚Šã™ã‚‹ã€Œè¦‹ã‹ã‘ä¸Šã®åœæ»ã€ãŒç™ºç”Ÿã—ã€ã‚·ã‚¹ãƒ†ãƒ ãŒèª¤ã£ã¦è¨“ç·´ã‚’æ—©æœŸçµ‚äº†ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
* åæŸåˆ¤æ–­ã®æ¨å¥¨ï¼šè¨“ç·´æ™‚é–“ã‚’å»¶é•·ã—ã€è¤‡æ•°ã®ãƒ•ã‚§ãƒ¼ã‚ºã§æå¤±ãŒç¶™ç¶šçš„ã«å®‰å®šã—ã¦ä¸‹è½ã—ã¦ã„ã‚‹ã‹ã«åŸºã¥ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®åæŸã‚’ç·åˆçš„ã«åˆ¤æ–­ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚è¨“ç·´æ™‚é–“ãŒä¸è¶³ã™ã‚‹ã¨ã€å¾©å…ƒåŠ¹æœãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

### ğŸ“ŠåŠ¹æœè©•ä¾¡
mT5-baseæ¨™æº–ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸåˆæœŸãƒ†ã‚¹ãƒˆã®æ¯”è¼ƒï¼š
* æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼šå°‚é–€åˆ†é‡ã®èªå½™å¾©å…ƒç‡ã¯æ¨å®š60%ä»¥ä¸‹ã€‚æ®‹ã‚Šã®40%ã¯è«–ç†ãŒæ··ä¹±ã—ã¦ãŠã‚Šã€æ¥­å‹™åˆ©ç”¨ã¯ã»ã¼ä¸å¯èƒ½ã§ã™ã€‚
* æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ˆã‚‹æ”¹å–„å¾Œï¼šå°‚é–€èªå½™ã®å¾©å…ƒç‡ã¯æ¨å®š85%ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã®15%ã®èª¤å·®ã®å¤§éƒ¨åˆ†ã¯æ„å‘³ã®è¿‘ã„èªå½™ã¸ã®ç½®æ›ã§ã‚ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®å¯èª­æ€§ã¨è«–ç†çš„ãªä¸€è²«æ€§ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚

### âš ï¸ä½¿ç”¨åˆ¶é™
* ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–­ç‰‡åŒ–ã®åˆ¶é™ï¼šãƒ¢ãƒ‡ãƒ«ãŒä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ã¯åˆ¶é™ãŒã‚ã‚Šã€ã¾ãŸå„ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆå†…ã§ãƒã‚¹ã‚¯ï¼ˆMaskï¼‰ã•ã‚Œã‚‹èªå½™æ•°ã‚‚é™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€é•·ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²å‡¦ç†ã™ã‚‹éš›ã«æ–‡è„ˆæƒ…å ±ãŒæ–­çµ¶ã—ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è·¨ãæ„å‘³ã‚’å®Œç’§ã«æ‰ãˆã‚‰ã‚Œãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ä¸€éƒ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å†åº¦å«ã‚ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
* ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é™ç•Œï¼šT5ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã®å¾©å…ƒã¯çµ±è¨ˆçš„ç¢ºç‡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ã€è¤‡é›‘ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹éš›ã«100%ã®å¾©å…ƒç²¾åº¦ã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ã¯ä¸å¯èƒ½ã§ã™ã€‚
* ãƒ‰ãƒ¡ã‚¤ãƒ³ä¾å­˜æ€§ï¼šå¾©å…ƒåŠ¹æœã¯ã€ã‚ã‚‰ã‹ã˜ã‚è¨­å®šã•ã‚ŒãŸå°‚é–€ç”¨èªé›†ã®ç¶²ç¾…æ€§ã¨æ·±ã•ã«å¼·ãä¾å­˜ã—ã¾ã™ã€‚

### ğŸŒŒä»Šå¾Œã®é–‹ç™ºè¨ˆç”»
* è‡ªå‹•æ¬ ææ¤œçŸ¥ï¼š
ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã€Œç•°å¸¸ãªæ–­ç‰‡ã€ã‚’éš ã‚ŒãŸä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚OCRèªè­˜ã«æ·±åˆ»ãªã‚ºãƒ¬ãŒç”Ÿã˜ãŸéš›ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç•°å¸¸ãªå¤‰å‹•ã‚’é€šã˜ã¦ã€æ„å‘³ã®æ–­çµ¶ç®‡æ‰€ã‚’è‡ªå‹•çš„ã«ç‰¹å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
* æ„å‘³ã®è‡ªå‹•ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼š
æ‰‹å‹•ã§æ¥ç¶šç‚¹ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãªãã€OCRã§æå‚·ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ãŒã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ä¿®å¾©ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

[Demo](#Demo)

---
<a name="deutsch"></a>
## Deutsch (Maschinelle Ãœbersetzung)
**T5-Refiner-DomainFocus** zielt darauf ab, dem Modell durch strategische Optimierung in der **Pre-Training-Phase** eine intrinsische â€semantische Resilienzâ€œ zu verleihen, damit es **Textdefekte robuster verarbeiten und Fachwissen aus spezifischen DomÃ¤nen injizieren kann.**

### ğŸ“– Projekthintergrund
Bei der **Digitalisierung medizinischer Archive** fÃ¼hrt **OCR (optische Zeichenerkennung)** aufgrund von beschÃ¤digtem Papier, StempelÃ¼berdeckungen usw. hÃ¤ufig zu â€Zeichendefektenâ€œ bei zentralen Fachbegriffen.
HerkÃ¶mmliche **T5-** oder **mT5-Modelle** (zusammenfassend T5) haben zwei Hauptprobleme bei der Verarbeitung dieser beschÃ¤digten Texte:
* Grenzen der zufÃ¤lligen Maskierung: Dies fÃ¼hrt dazu, dass das Modell nur lernt, WÃ¶rter basierend auf WortstÃ¤mmen zu â€ratenâ€œ, anstatt medizinische Konzepte wirklich vollstÃ¤ndig zu verstehen.
* Tokenisierungs-Fehlausrichtung: Wenn Buchstaben in Fachbegriffen fehlen, zerlegt der Tokenizer diese in bedeutungslose Fragmente, wodurch das Modell seinen semantischen Fokus verliert.

### âœ… Aktuelle Kernfunktionen
Dieses Projekt verlÃ¤sst sich derzeit nicht auf komplexe Hardcoding-Regeln, sondern stÃ¤rkt die ModellfÃ¤higkeiten durch die Optimierung des Daten-Preprocessing-Workflows:

* Atomare Maskierung gesteuert durch Experten-Vokabular:
Auf Basis eines benutzerdefinierten Vokabulars wird das Modell gezwungen, Fachbegriffe (z. B. akuter Vorderwandmyokardinfarkt) als untrennbare Einheit zu maskieren. Auf diese Weise wird das Modell gezwungen, Antworten aus der Logik des Kontextes zu finden, anstatt durch verbleibende Zeichen oberflÃ¤chliche RÃ¼ckschlÃ¼sse zu ziehen.

* VerstÃ¤rktes Training durch manuelle Einstellungen:
UnterstÃ¼tzt die manuelle ErhÃ¶hung der Maskierungswahrscheinlichkeit fÃ¼r spezifische, hochgradig schwierige Begriffe (ğŸ’¡ empfohlen bei 50%-70%, nicht Ã¼ber 80%), wÃ¤hrend gleichzeitig die allgemeine Maskierungsrate (20%-25%) synchron erhÃ¶ht werden kann.

* Automatische Vermeidung von Satzzeichen:
Verhindert die EinfÃ¼hrung von StÃ¶rfaktoren.

Durch die kÃ¼nstliche Erzeugung von Szenarien mit â€extremem Informationsverlustâ€œ wird das Modell gezwungen, selbst bei schlechtesten Eingabebedingungen eine prÃ¤zise Wiederherstellung der Fachsemantik beizubehalten.

### â—ï¸ Hinweise zum Training
* Vorzeitigen Stopp des Modells verhindern: Nach dem Preprocessing kann es bei T5-Modellen zu einer TÃ¤uschung durch langsam sinkenden Loss oder lokale Schwankungen kommen, was dazu fÃ¼hrt, dass das System das Training fÃ¤lschlicherweise vorzeitig stoppt.
* Empfehlung zur Konvergenzbeurteilung: Es wird empfohlen, die Trainingsdauer zu erhÃ¶hen und die Konvergenz des Modells basierend auf dem kontinuierlichen und stabilen Sinken des Loss Ã¼ber mehrere Phasen hinweg umfassend zu beurteilen. Bei unzureichender Trainingszeit kann der Wiederherstellungseffekt stark beeintrÃ¤chtigt werden.

### ğŸ“Š EffektivitÃ¤tsbewertung
Basierend auf vorlÃ¤ufigen Vergleichstests im mT5-base Standardmodell:
* Leistung des Standardmodells: Die Wiederherstellungsrate von Fachvokabular wird auf unter 60% geschÃ¤tzt, wobei die restlichen 40% logisch verwirrend und fÃ¼r den geschÃ¤ftlichen Einsatz kaum akzeptabel sind.
* Nach der Verbesserung durch dieses Projekt: Die Wiederherstellungsrate von Fachvokabular erreichte geschÃ¤tzte 85%. Von den verbleibenden 15% Fehlerquote entfÃ¤llt der GroÃŸteil auf semantisch Ã¤hnliche Wortsubstitutionen, was die allgemeine Lesbarkeit und logische KohÃ¤renz des Textes erheblich verbessert.

### âš ï¸ NutzungseinschrÃ¤nkungen
* EinschrÃ¤nkung durch Kontext-Fragmentierung: Da die TextlÃ¤nge pro Verarbeitungsschritt begrenzt ist und die Anzahl der maskierten WÃ¶rter pro Textsegment limitiert ist, kann es bei der Aufteilung langer Dokumente zu BrÃ¼chen in den Kontextinformationen kommen. Dies fÃ¼hrt dazu, dass einige segmentÃ¼bergreifende Semantiken nicht perfekt erfasst werden kÃ¶nnen. Es wird empfohlen, Teile des Kontextes fÃ¼r das Re-Training zurÃ¼ckzugeben.
* Algorithmische Grenzen: Da die Wiederherstellung des T5-Modells auf statistischen Wahrscheinlichkeitsalgorithmen basiert, kann eine 100%ige Genauigkeit bei komplexen Texten nicht garantiert werden.
* DomÃ¤nenabhÃ¤ngigkeit: Der Wiederherstellungseffekt hÃ¤ngt stark von der Abdeckung und Tiefe des vordefinierten Experten-Vokabulars ab.

### ğŸŒŒ ZukÃ¼nftige EntwicklungsplÃ¤ne
* Automatische Defekterkennung:
Nutzung â€anormaler Fragmenteâ€œ des Tokenizers als implizite Signale. Wenn OCR-Erkennungen schwerwiegende Fehlausrichtungen aufweisen, kann das Modell Ã¼ber abnormale Schwankungen in der Token-Sequenz semantische BrÃ¼che automatisch lokalisieren.
* Automatische semantische Ausrichtung:
End-to-End-Reparatur von OCR-beschÃ¤digten Texten durch das Modell, ohne dass manuell VerknÃ¼pfungspunkte angegeben werden mÃ¼ssen.

[Demo](#Demo)

---
<a name="francais"></a>
## FranÃ§ais (Traduction automatique)
**T5-Refiner-DomainFocus** vise Ã  doter le modÃ¨le d'une Â« rÃ©silience sÃ©mantique Â» intrinsÃ¨que grÃ¢ce Ã  l'optimisation des stratÃ©gies lors de la **phase de prÃ©-entraÃ®nement**, **lui permettant de gÃ©rer plus solidement les lacunes textuelles et d'injecter une expertise mÃ©tier.**

### ğŸ“– Contexte du projet
Lors de la **numÃ©risation d'archives mÃ©dicales**, l'**OCR (Reconnaissance Optique de CaractÃ¨res)** entraÃ®ne souvent des Â« lacunes de caractÃ¨res Â» dans les termes clÃ©s en raison de dommages sur le papier ou de l'obstruction par des tampons.
Les modÃ¨les **T5** ou **mT5** conventionnels (collectivement appelÃ©s T5) prÃ©sentent deux problÃ¨mes majeurs lors du traitement de ces textes endommagÃ©s :
* Limites du masquage alÃ©atoire : Le modÃ¨le apprend uniquement Ã  Â« deviner Â» les mots Ã  partir des racines, sans vÃ©ritablement comprendre les concepts mÃ©dicaux complets.
* ProblÃ¨me de dÃ©salignement de la tokenisation : Lorsqu'un terme perd des lettres, le tokenizer le fragmente en morceaux dÃ©nuÃ©s de sens, faisant perdre au modÃ¨le son centre de gravitÃ© sÃ©mantique.

### âœ… Fonctions clÃ©s actuelles
Ce projet ne repose pas sur des rÃ¨gles codÃ©es en dur complexes, mais renforce les capacitÃ©s du modÃ¨le en optimisant le flux de prÃ©traitement des donnÃ©es :

* Masquage atomique guidÃ© par un lexique d'experts :
S'appuyant sur un lexique personnalisÃ©, il force le modÃ¨le Ã  considÃ©rer les termes techniques (ex : infarctus aigu du myocarde de la paroi antÃ©rieure) comme un tout indivisible lors du masquage. De cette maniÃ¨re, le modÃ¨le est contraint de chercher des rÃ©ponses dans la logique du contexte plutÃ´t que de spÃ©culer sur des caractÃ¨res rÃ©siduels.

* EntraÃ®nement renforcÃ© par paramÃ©trage manuel :
Permet d'augmenter manuellement la probabilitÃ© de masquage de certains termes particuliÃ¨rement difficiles (ğŸ’¡ recommandÃ© entre 50% et 70%, ne pas dÃ©passer 80%), tout en augmentant simultanÃ©ment le taux de masquage global (20%-25%).

* Ã‰vitement automatique de la ponctuation :
EmpÃªche l'introduction d'interfÃ©rences.

En crÃ©ant artificiellement des scÃ©narios de Â« perte d'information extrÃªme Â», le modÃ¨le est contraint de maintenir une restitution prÃ©cise de la sÃ©mantique professionnelle, mÃªme dans les pires conditions d'entrÃ©e.

### â—ï¸ PrÃ©cautions d'entraÃ®nement
* PrÃ©venir l'arrÃªt prÃ©maturÃ© du modÃ¨le : AprÃ¨s le prÃ©traitement, le modÃ¨le T5 peut donner l'illusion d'une baisse lente de la perte (Loss) ou de fluctuations locales, ce qui peut amener le systÃ¨me Ã  arrÃªter l'entraÃ®nement prÃ©maturÃ©ment par erreur.
* Conseils pour juger de la convergence : Il est recommandÃ© d'augmenter la durÃ©e d'entraÃ®nement et de juger de la convergence de maniÃ¨re globale en vÃ©rifiant si la perte continue de descendre de faÃ§on stable sur plusieurs Ã©tapes. Si le temps d'entraÃ®nement est insuffisant, l'effet de restauration pourrait Ãªtre considÃ©rablement rÃ©duit.

### ğŸ“Š Ã‰valuation des rÃ©sultats
Selon les tests comparatifs prÃ©liminaires sur le modÃ¨le standard mT5-base :
* Performance du modÃ¨le standard : Le taux de restauration du vocabulaire spÃ©cialisÃ© est estimÃ© Ã  moins de 60 %, les 40 % restants Ã©tant logiquement confus et pratiquement inacceptables pour une utilisation mÃ©tier.
* AprÃ¨s amÃ©lioration par ce projet : Le taux de restauration du vocabulaire spÃ©cialisÃ© atteint environ 85 %. Parmi les 15 % d'erreurs restantes, la plupart sont des substitutions par des termes sÃ©mantiquement proches, ce qui amÃ©liore considÃ©rablement la lisibilitÃ© globale et la cohÃ©rence logique du texte.

### âš ï¸ Limites d'utilisation
* Limitation de la fragmentation du contexte : En raison de la longueur limitÃ©e du texte traitÃ© en une seule fois et du nombre restreint de mots masquÃ©s (Mask) par segment, le traitement de documents longs peut entraÃ®ner une rupture des informations contextuelles, empÃªchant la capture parfaite de la sÃ©mantique entre les paragraphes. Il est recommandÃ© de rÃ©injecter une partie du contexte pour le rÃ©entraÃ®nement.
* Limites algorithmiques : La restauration du modÃ¨le T5 Ã©tant basÃ©e sur des algorithmes de probabilitÃ© statistique, il est impossible de garantir une prÃ©cision de restauration de 100 % lors du traitement de textes complexes.
* DÃ©pendance au domaine : L'efficacitÃ© de la restauration dÃ©pend fortement de la couverture et de la profondeur du lexique d'experts prÃ©dÃ©fini.

### ğŸŒŒ Plan de dÃ©veloppement futur
* Perception automatique des lacunes :
Utiliser les Â« fragments anormaux Â» du tokenizer comme signaux implicites. En cas de dÃ©calage grave de l'OCR, le modÃ¨le pourra localiser automatiquement les ruptures sÃ©mantiques via les fluctuations anormales de la sÃ©quence de tokens.
* Alignement sÃ©mantique automatique :
RÃ©aliser une rÃ©paration de bout en bout des textes endommagÃ©s par l'OCR sans avoir besoin de spÃ©cifier manuellement les points de jonction.

[Demo](#Demo)

---
<a name="Demo"></a>
## ğŸ“¡ Demo

---
<a name="Requirements"></a>
## ğŸ› ï¸ Requirements

```text


```
---
<a name="References"></a>
## ğŸ’ªReferences / Citation
```markdown
This project builds upon the T5 or mT5. If you use mT5, please cite:

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

If you use this project, please cite it as:

@misc{llap4585,
    title={{T5-Refiner-DomainFocus}: Injecting domain expertise into T5 via precision vocabulary-guided masking.},
    author={llap4585},
    howpublished = {\url{https://github.com/llap4585/T5-Refiner-DomainFocus}},
    year={2026}
}

```
---

<a name="Privacy"></a>
## ğŸ›¡ï¸ Privacy & Security

**Local Processing Only:** This tool performs all operations locally on your machine. No medical reports, patient data, or sensitive information are uploaded to any external servers or cloud services. Your data remains under your control at all times.

**Third-party Disclaimer:** All third-party libraries required for operation are provided by the user's environment. These dependencies and their components are not under the management or control of this project.

**ä»…é™æœ¬åœ°å¤„ç†ï¼š** æœ¬å·¥å…·çš„æ‰€æœ‰æ“ä½œå‡åœ¨æ‚¨çš„æœ¬åœ°è®¡ç®—æœºä¸Šæ‰§è¡Œã€‚ä¸ä¼šå°†ä»»ä½•åŒ»ç–—æŠ¥å‘Šã€æ‚£è€…æ•°æ®æˆ–æ•æ„Ÿä¿¡æ¯ä¸Šä¼ åˆ°ä»»ä½•å¤–éƒ¨æœåŠ¡å™¨æˆ–äº‘æœåŠ¡ã€‚æ‚¨çš„æ•°æ®å§‹ç»ˆç”±æ‚¨æŒæ§ã€‚

**ç¬¬ä¸‰æ–¹åº“å£°æ˜ï¼š** æœ¬å·¥å…·è¿è¡Œæ‰€ä¾èµ–çš„æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“å‡ç”±ç”¨æˆ·ç¯å¢ƒæä¾›ï¼Œè¿™äº›ç¬¬ä¸‰æ–¹åº“åŠå…¶ç›¸å…³ç»„ä»¶ä¸åœ¨æœ¬é¡¹ç›®çš„ç®¡ç†ä¸æ§åˆ¶èŒƒå›´å†…ã€‚


---
> **âš ï¸Disclaimer:** The non-English and non-Chinese versions of this documentation are provided for convenience only and were generated using machine translation. In case of any discrepancy, the Chinese version shall prevail.
